

# ğŸ•µï¸â€â™‚ï¸ **Reconnaissance (Footprinting)**

*â€œThe art of gathering intelligence before launching an attack or penetration test.â€*

---

## ğŸ” **What is Reconnaissance?**

Reconnaissance is the **first phase** in ethical hacking or cyberattacks. The goal is to **collect as much information as possible** about the target **without detection**, to plan the attack or security assessment.

---

## ğŸ“‚ **Types of Reconnaissance**

| Type | Description | Examples |
| --- | --- | --- |
| ğŸ”¹ **Passive Reconnaissance** | No direct interaction with target | WHOIS, Google, social media, DNS records |
| ğŸ”¸ **Active Reconnaissance** | Directly interacting with the target | Nmap scans, phishing, web probing |

---

## ğŸ› ï¸ **Key Areas of Focus**

### 1ï¸âƒ£ **Network Vulnerabilities**

- ğŸ”“ **Open Ports**: Exposed services (e.g., SSH, FTP)
- âš ï¸ **Misconfigured Devices**: Default settings on routers/firewalls
- ğŸ•¸ï¸ **Weak Protocols**: Use of outdated (Telnet, FTP)
- ğŸšª **Unauthorized Access**: No/weak authentication on systems

### 2ï¸âƒ£ **Web Application Vulnerabilities**

- âš”ï¸ OWASP Top 10 issues like:
    - **SQL/Command Injections**
    - **Broken Authentication**
    - **XSS / CSRF**
    - **Insecure Deserialization**

### 3ï¸âƒ£ **System/Server Vulnerabilities**

- ğŸ”§ **Unpatched Software**
- ğŸ—ï¸ **Default Credentials**
- â¬†ï¸ **Privilege Escalation**
- ğŸ—ƒï¸ **Insecure File Permissions**

---

## ğŸ¢ **Organization Information Sources**

| Source | Information |
| --- | --- |
| ğŸŒ Company Website | General info, services, internal links |
| ğŸ§‘â€ğŸ’¼ Employee Profiles | From LinkedIn, public platforms |
| ğŸ¢ Office Details | Locations, phone numbers |
| ğŸ’¬ HTML Comments | â€œTODOâ€, developer hints |
| ğŸ“° Press Releases | Recent updates/expansions |

---

## ğŸŒ **Network Information**

- ğŸ”— **Domain Names/Subdomains**: e.g., `example.com`, `intranet.example.com`
- ğŸ§® **IP Addresses**: Public/private ranges
- ğŸ“¶ **TCP/UDP Services**: HTTP (80), SSH (22), etc.
- ğŸ” **VPN/IDS/IPS**: Look for exposed configurations
- ğŸ“ **VoIP Systems**: For social engineering

---

## ğŸ’» **Operating System Info**

- ğŸ‘¥ **Users/Groups**: Naming conventions reveal roles
- ğŸ§¾ **Banner Grabbing**: Software/version info
- ğŸ§­ **Routing Tables**
- ğŸ“¡ **SNMP**: Network device details
- ğŸ—ï¸ **System Architecture**: 32-bit or 64-bit
- ğŸ“› **System Names**: e.g., `SQLServer`, `DC01`

---

## ğŸŒ **External Reconnaissance Techniques**

| Method | Description |
| --- | --- |
| ğŸ“ Phone Numbers | Leaked or public numbers |
| ğŸ§  Google Hacking | `filetype:`, `inurl:`, `site:` queries |
| ğŸ•·ï¸ Website Mirroring | Using `wget`, `HTTrack` |
| ğŸ“¦ GitHub Recon | Tools: Gitrob, truffleHog |
| ğŸ—„ï¸ Archive Sites | Archive.org snapshots |
| ğŸ“¬ Email Headers | Reveal IPs, server paths |
| ğŸ” WHOIS Lookup | Domain owner & registrar info |

---

## ğŸ§ª **Internal Reconnaissance**

- ğŸ–§ **Internal IPs/DNS Records**
- ğŸŒ **Private Websites**: Intranet portals
- ğŸ—‘ï¸ **Dumpster Diving**: Old docs, USBs, sticky notes
- ğŸ‘€ **Shoulder Surfing**: Watching users type passwords

---

# ğŸŒ **Website Reconnaissance**

### ğŸ§  Why Perform Website Recon?

- ğŸ›¡ï¸ **Security Assessment**
- ğŸ“Š **Competitive Analysis**
- ğŸ” **SEO/Marketing Insights**
- âœ… **Compliance Checks**

### ğŸ” Typical Activities

- ğŸŒ **Identify IP, domain, SSL cert**
- âš™ï¸ **Tech Stack**: CMS, JS frameworks (via BuiltWith)
- ğŸ”’ **Find exposed info**: emails, phone numbers
- ğŸ•¸ï¸ **Spidering**: Map links & structure
- ğŸ•°ï¸ **Historical Snapshots**: Wayback Machine

### ğŸ§° Key Tools

| Tool Type | Examples |
| --- | --- |
| ğŸŒ Online Tools | Shodan, Censys, BuiltWith |
| ğŸ§© Crawlers | Screaming Frog, XML Sitemaps |
| ğŸ§¾ CLI Tools | wget, HTTrack |
| ğŸ—ƒï¸ Archive | Archive.org, Archive.ph |

---

## ğŸ“‚ **Website Files to Investigate**

| File | Purpose |
| --- | --- |
| `sitemap.html` | Human-readable navigation |
| `sitemap.xml` | For search engine indexing |
| `robots.txt` | Controls crawler access |

---

## ğŸ§ª **Example Footprint Checklist**

1. âœ… Visit Website
2. ğŸ‘¥ Gather employee info
3. ğŸ—ºï¸ Check location/address
4. ğŸ“§ Find contact details
5. ğŸ§¾ Examine sitemap and robots.txt
6. ğŸ§° Use Shodan/Censys for tech info
7. ğŸ“š Search Archive.org for old versions
8. ğŸ” Google Dorking
9. ğŸ“¦ Look on GitHub for exposed code
10. ğŸ“¥ Analyze email headers if available

Would you like a printable PDF version or visual mind map of this too?

---

Here's your professionally formatted and **comprehensive notes** on `sitemap.html`, `sitemap.xml`, and `robots.txt`, including **real-life examples**, **tools**, and **differences in spidering vs crawling** â€” in a clear, attractive structure.

---

# ğŸŒ Sitemap, Robots.txt, and Crawling Explained

## ğŸ“ 1. **Sitemap Overview**

### ğŸ—‚ï¸ `sitemap.html` â€“ *User-Facing Index*

- ğŸ“Œ A human-readable **HTML page**.
- Acts as a **"table of contents"** for website visitors.
- Helps users find key sections/pages easily.

**ğŸ§­ Real Examples**:

- [India TV Sitemap](https://www.indiatv.in/cms/sitemap.html)
- [MCA India Sitemap](https://www.mca.gov.in/MinistryV2/sitemap.html)
- [Telangana Transport Sitemap](https://www.transport.telangana.gov.in/html/sitemap.html)

---

### ğŸ”— `sitemap.xml` â€“ *Search Engine Bot Index*

- ğŸ“Œ A machine-readable **XML file**.
- Guides search engines to **index** specific pages.
- Useful for large or frequently updated websites.

**ğŸ¯ Key Purpose**:

- Helps bots (e.g., Googlebot) find all important URLs.

**ğŸ›  Online Generator**:

- [XML Sitemap Generator](https://www.xml-sitemaps.com/)

---

### ğŸš« `robots.txt` â€“ *Crawler Restrictions*

- ğŸ“Œ A plain text file placed at root (`example.com/robots.txt`)
- Tells **search engines what to avoid** indexing.

**Example Use Cases**:

- Block `/admin/` or private areas
- Avoid indexing duplicate content

**Syntax Example**:

```
User-agent: *
Disallow: /private/
Allow: /public/

```

---

## ğŸ•·ï¸ 2. **Spidering vs Crawling** â€“ Whatâ€™s the Difference?

| Concept | Description | Key Focus | Tools |
| --- | --- | --- | --- |
| **Spidering** | Following links recursively to **map site structure** | Link discovery & structure | Screaming Frog, Burp Suite |
| **Crawling** | Visiting pages, downloading data, and following links | Data collection & analysis | Googlebot, Katana, HTTrack |

ğŸ§  In **cybersecurity recon**, both are used to discover content & vulnerabilities.

---

## ğŸ§ª 3. **Recon Tools for Sitemap & Crawling**

### ğŸ”§ Common Tools:

| Tool | Purpose |
| --- | --- |
| ğŸ•·ï¸ Screaming Frog | Website spidering & SEO auditing |
| ğŸ”„ ConvertCSV URL Extractor | Extract URLs from site data |
| ğŸ§  Katana | Recon spidering & vulnerability checking |
| ğŸ“¥ Wget | Mirror websites and inspect HTML |
| ğŸ“¡ HTTrack | Full site clone (offline browsing) |

---

### âš™ï¸ Example: **Wget Usage**

```bash
wget https://example.com

```

âœ”ï¸ Downloads the page and resources locally.

---

### âš™ï¸ Example: **HTTrack Usage**

```bash
apt install httrack
httrack

```

âœ”ï¸ Launches an interactive website mirroring session.

---

## ğŸ•°ï¸ 4. **View Historical Versions â€“ Wayback Machine**

| Site | Use |
| --- | --- |
| ğŸŒ [Archive.org](https://archive.org/web/) | View past versions of websites |
| ğŸ“œ Orkut Archive | Legacy archives (if available) |
| ğŸ–¼ï¸ Archive.ph | Snapshot of websites |

---

## âœ… Summary Table

| File | Purpose | Audience | Example |
| --- | --- | --- | --- |
| `sitemap.html` | Navigation | Human users | Visitors browsing site |
| `sitemap.xml` | Indexing | Search bots | Google indexing |
| `robots.txt` | Restrictions | Search bots | Block private pages |

---

Let me know if you want these notes exported as a **PDF**, **Markdown**, or ready for **GitHub formatting**!
